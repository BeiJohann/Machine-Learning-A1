{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo 3 â€” next word prediction via LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a class for loading the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import random\n",
    "\n",
    "class TextLoader(object):\n",
    "    def __init__(self, filename):\n",
    "        thefile = open(filename, \"r\")\n",
    "        \n",
    "        tokenizer = WordPunctTokenizer()\n",
    "        \n",
    "        self.sentences = []\n",
    "        self.predicted = []\n",
    "        for line in thefile:\n",
    "            sentence = tokenizer.tokenize(line)\n",
    "            if sentence:\n",
    "                # List of tokenized sentences with the start symbol\n",
    "                # and parallel list of each word's next word and\n",
    "                # the end symbol.\n",
    "                self.sentences.append([\"<s>\"] + sentence)\n",
    "                self.predicted.append(sentence + [\"<e>\"])\n",
    "        \n",
    "\n",
    "        uniquevocab = list(set(sum(self.sentences, []))) + [\"<e>\"]\n",
    "        self.vocabindex = {}\n",
    "        for i in range(len(uniquevocab)):\n",
    "            self.vocabindex[uniquevocab[i]] = i\n",
    "        \n",
    "        # Turn the sentences into lists of indices into the vocab.\n",
    "        self.int_sentences = [[self.vocabindex[x] for x in y] for \n",
    "                              y in self.sentences]\n",
    "        self.int_predicted = [[self.vocabindex[x] \n",
    "                               for x in y] for \n",
    "                              y in self.predicted]\n",
    "        self.pairs = list(zip(self.int_sentences, self.int_predicted))\n",
    "    \n",
    "    # Get it randomized at every round.\n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.pairs)\n",
    "        int_sentences, int_predicted = zip(*self.pairs)\n",
    "        self.int_sentences = list(int_sentences)\n",
    "        self.int_predicted = list(int_predicted)\n",
    "    \n",
    "    def __getitem__(self, n):\n",
    "        return self.pairs[n]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b2eec98a133c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TextLoader(\"snark.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1924, 1448, 138, 2258, 2046, 748, 138, 2258, 1411, 878, 415, 1851],\n",
       " [1448, 138, 2258, 2046, 748, 138, 2258, 1411, 878, 415, 1851, 2267])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every item in the `TextLoader` is now a pair consisting of the sentence, and the next words for each word.  Note that 64 is the start symbol and 2267 is the end symbol.\n",
    "\n",
    "### Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMFun(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, lstm_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, input_size)\n",
    "        # We need at least two LSTM layers for the LSTM dropout to be meaningful.\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=lstm_layers, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        # The dimension that we run the softmax on is really important.  We want the dimension that\n",
    "        # corresponds to a distribution over the vocabulary.\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        init_hidden = self.init_hidden(len(sentence[0]))\n",
    "        \n",
    "        # The sentence as indices goes directly into the embedding layer,\n",
    "        # which selects randomly-initialized vectors corresponding to the\n",
    "        # indices.\n",
    "        output = self.emb(sentence)\n",
    "        output, hidden = self.lstm(output, init_hidden)\n",
    "        output = self.linear(output)\n",
    "        return self.softmax(output)\n",
    "    \n",
    "    def set_dev(self, dev):\n",
    "        self.dev = dev\n",
    "    \n",
    "    # We need an initial pair of hidden weights and memory state.\n",
    "    def init_hidden(self, sen_len):\n",
    "        return (torch.zeros(self.lstm_layers, sen_len, self.hidden_size).to(self.dev),\n",
    "                torch.zeros(self.lstm_layers, sen_len, self.hidden_size).to(self.dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the forward pass to make sure we have the dimensions right.\n",
    "\n",
    "Remember that we have never called `shuffle` so everything should be in the file order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = tl[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence[0]) # Remember that each sentence is a pair whose first member is the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMFun(len(tl.vocabindex), 300, 200)\n",
    "model.set_dev(torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're only seeing batch sizes of 1. So the hidden layer needs to be initialized to the length of that single sentence, which is 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = model.init_hidden(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 200])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do the hidden weights come in pairs? Because it is a two-layer network, so we need to feed an initial state with two layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.emb(torch.LongTensor([sentence[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is expecting batches of sentences, so we need to enclose the single sentence in an outer dimension to represent a batch of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2732,  0.7312, -0.4062,  ..., -0.1858,  0.0316,  0.3229],\n",
       "         [ 1.7671, -1.3043, -0.0223,  ..., -1.0204,  1.2957, -0.1316],\n",
       "         [ 0.7743, -1.4921, -0.3188,  ...,  0.0933, -0.5053,  1.0362],\n",
       "         ...,\n",
       "         [-0.8322, -1.4579,  0.7607,  ...,  1.4547,  1.1233, -2.9324],\n",
       "         [ 0.5830, -0.3188, -2.6546,  ...,  0.4145,  0.4094,  1.1061],\n",
       "         [-0.6901, -0.1143, -0.0835,  ...,  1.7085,  0.0943, -1.2241]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 300])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2, hidden2 = model.lstm(output, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 12, 200]), torch.Size([2, 12, 200]), torch.Size([2, 12, 200]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2.size(), hidden2[0].size(), hidden2[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output3 = model.linear(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0235, -0.0082,  0.0413,  ...,  0.0039,  0.0900,  0.0044],\n",
       "         [-0.0186, -0.0338,  0.0847,  ...,  0.0130,  0.0580,  0.0289],\n",
       "         [-0.0251, -0.0095,  0.0593,  ..., -0.0152,  0.0425,  0.0078],\n",
       "         ...,\n",
       "         [-0.0128,  0.0034,  0.0747,  ..., -0.0004,  0.0801,  0.0282],\n",
       "         [-0.0299,  0.0040,  0.0498,  ..., -0.0129,  0.0791,  0.0272],\n",
       "         [-0.0193, -0.0144,  0.0602,  ...,  0.0071,  0.0572,  0.0212]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 2268])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output3.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from 300-dimension embeddings to a pair of 200-dimensional LSTM layers, and then the linear layer \"decompresses\" the 200 dimensions to the full size of the vocabulary, 2268.  Note that there is a batch dimension, but the batch size is 1, just as we started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaloutput = model.softmax(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 2268])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaloutput.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remember that there's a batch size of 1. We have to use `finaloutput[0]` to access the 12 vectors that correspond to probability distributions for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2268"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tl.vocabindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-7.7509, -7.7356, -7.6861,  ..., -7.7234, -7.6373, -7.7230],\n",
       "         [-7.7459, -7.7611, -7.6426,  ..., -7.7142, -7.6692, -7.6984],\n",
       "         [-7.7524, -7.7368, -7.6680,  ..., -7.7425, -7.6848, -7.7196],\n",
       "         ...,\n",
       "         [-7.7402, -7.7240, -7.6527,  ..., -7.7277, -7.6473, -7.6992],\n",
       "         [-7.7572, -7.7232, -7.6775,  ..., -7.7402, -7.6482, -7.7001],\n",
       "         [-7.7466, -7.7417, -7.6671,  ..., -7.7202, -7.6701, -7.7061]]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaloutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2268])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaloutput[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, we applied LogSoftmax...\n",
    "finalprobs = torch.exp(finaloutput[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0004, 0.0004, 0.0005,  ..., 0.0004, 0.0005, 0.0004],\n",
       "        [0.0004, 0.0004, 0.0005,  ..., 0.0004, 0.0005, 0.0005],\n",
       "        [0.0004, 0.0004, 0.0005,  ..., 0.0004, 0.0005, 0.0004],\n",
       "        ...,\n",
       "        [0.0004, 0.0004, 0.0005,  ..., 0.0004, 0.0005, 0.0005],\n",
       "        [0.0004, 0.0004, 0.0005,  ..., 0.0004, 0.0005, 0.0005],\n",
       "        [0.0004, 0.0004, 0.0005,  ..., 0.0004, 0.0005, 0.0005]],\n",
       "       grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(finalprobs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a true probability distribution, since it sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() # Takes the LogSoftmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that we started with sentence 300.\n",
    "target = sentence[1]\n",
    "target = torch.LongTensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1448,  138, 2258, 2046,  748,  138, 2258, 1411,  878,  415, 1851, 2267])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little confusing. `nn.NLLLoss` takes as the target the *indices* of the output words; it constructs the one-hot vectors/distributions itself. Easy to forget if you're used to sklearn, keras, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(finaloutput[0], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.7129, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're not actually doing the loop, we don't backpropagate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(tl, epochs=3):\n",
    "    model = LSTMFun(len(tl.vocabindex), 300, 200)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    dev = torch.device('cuda:3')\n",
    "    model = model.to(dev)\n",
    "    model.set_dev(dev)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        tl.shuffle() # Shuffle at every epoch.\n",
    "                \n",
    "        for input_sent, output_sent in tl: \n",
    "            # This is extremely inefficient, a waste of GPU to send \n",
    "            # each instance individually, since we're not using\n",
    "            # batching.\n",
    "            input_tensor = torch.LongTensor(np.array([input_sent])).to(dev)\n",
    "            output_tensor = torch.LongTensor(np.array(output_sent)).to(dev)\n",
    "            optimizer.zero_grad()\n",
    "            result = model(input_tensor)\n",
    "            loss = criterion(result[0], output_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = train(tl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to obtain the next word of a prefix sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, prefix):\n",
    "    dev = torch.device(\"cuda:3\")\n",
    "    indices = [tl.vocabindex[word] for word in prefix.split()]\n",
    "    indices = torch.LongTensor(np.array([indices])).to(dev)\n",
    "    result = model(indices)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test it, we need to use strictly words that will be in the vocabulary, otherwise it will raise an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(trained, \"<s> Baker believes woe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2267"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0][-1].topk(1)[1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vocab = {v: k for k, v in tl.vocabindex.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<e>'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_vocab[2267]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It predicted the end symbol. But does it always do that? In the first place, it's been trained on very little data and with very few epochs, so it's quite possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, prefix, length=6):\n",
    "    dev = torch.device(\"cuda:3\")\n",
    "    \n",
    "    for i in range(length):\n",
    "        # Iteratively get the next several words by concatenating\n",
    "        # predictions.\n",
    "        indices = [tl.vocabindex[word] for word in prefix.split()]\n",
    "        indices = torch.LongTensor(np.array([indices])).to(dev)\n",
    "        result = model(indices)\n",
    "        nextword = rev_vocab[result[0][-1].topk(1)[1].item()]\n",
    "        prefix = prefix + \" \" + nextword\n",
    "        \n",
    "    return prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> believes the profit and hope ; <e> , <e> .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(trained, \"<s> believes the profit and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Captain unfailing and also a Snark , <e> , <e>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(trained, \"the Captain unfailing and also\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'friend staff <s> quivering dens corrupt , <e> , <e> . <e>'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(trained, \"friend staff <s> quivering dens corrupt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it does behave creatively. Let's train another model for a few more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained2 = train(tl, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> believes the profit and hope ; <e> , <e> ,'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(trained2, \"<s> believes the profit and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Captain unfailing and also seems a Snark , <e> ,'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(trained2, \"the Captain unfailing and also\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'friend staff <s> quivering dens corrupt data , <e> , <e> ,'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(trained2, \"friend staff <s> quivering dens corrupt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gives us more, but it's still too little data to be very good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
